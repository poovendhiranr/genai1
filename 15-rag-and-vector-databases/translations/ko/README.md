# 검색 증강 생성 (RAG) 및 벡터 데이터베이스

[![검색 증강 생성 (RAG) 및 벡터 데이터베이스](../../images/15-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst)

검색 응용 프로그램 수업에서는 대규모 언어 모델(LLM)에 사용자의 데이터를 통합하는 방법을 간략히 배웠습니다. 이번 수업에서는 LLM 응용 프로그램에 데이터를 결합하는 개념, 프로세스의 메커니즘 및 임베딩과 텍스트를 포함한 데이터 저장 방법에 대해 더 자세히 알아보겠습니다.

> **동영상 곧 공개 예정**

## 소개

이 강의에서는 다음 내용을 다룹니다:

- RAG의 소개, RAG가 무엇이며 AI(인공지능)에서 왜 사용되는지에 대한 설명.

- 벡터 데이터베이스가 무엇인지 이해하고 우리의 애플리케이션에 맞는 벡터 데이터베이스를 만드는 방법.

- 애플리케이션에 RAG를 통합하는 실용적 예제.

## 학습 목표

이 수업을 완료한 후, 여러분은 다음을 할 수 있습니다:

- 데이터 검색 및 처리에서 RAG의 중요성을 설명합니다.

- RAG 애플리케이션을 설정하고 데이터를 LLM에 연결합니다.

- RAG와 벡터 데이터베이스를 LLM 애플리케이션에 효과적으로 통합합니다.

## 우리의 시나리오: 우리만의 데이터를 이용해 LLM 강화하기

이번 강의에서는 교육 스타트업에 우리의 노트를 추가하여 챗봇이 다양한 주제에 대해 더 많은 정보를 얻을 수 있도록 하고자 합니다. 우리가 가진 노트를 사용하면 학습자들은 더 잘 공부하고 다양한 주제를 이해할 수 있어 시험 준비가 쉬워질 것입니다. 이러한 시나리오를 만들기 위해 우리는 다음을 사용할 것입니다:

- `Azure OpenAI:` 챗봇을 만들기 위해 사용할 LLM
- `AI for beginners' lesson on Neural Networks`: 우리의 LLM을 위한 데이터
- `Azure AI Search` 와 `Azure Cosmos DB:` 데이터를 저장하고 검색 인덱스를 만들기 위한 벡터 데이터베이스

사용자들은 노트로부터 연습 퀴즈, 복습 플래시 카드 등을 만들 수 있으며, 노트를 간결하게 요약할 수도 있습니다. 시작하기에 앞서, RAG가 무엇이며 어떻게 작동하는지 살펴보겠습니다.

## 검색 증강 생성 (RAG)

LLM 기반 챗봇은 사용자 프롬프트를 처리하여 응답을 생성합니다. 이 챗봇은 다양한 주제에 대해 상호 작용하며 사용자와 소통하기 위해 설계되었습니다. 그러나 응답은 제공된 맥락과 기본 훈련 데이터에 한정됩니다. 예를 들어, GPT-4의 지식 데이터 컷오프는 2021년 9월까지로 되어 있어, 이후에 발생한 사건에 대해서는 지식이 없습니다. 또한, LLM을 훈련시키는 데 사용된 데이터에는 개인 노트나 회사의 제품 매뉴얼과 같은 기밀 정보가 포함되지 않습니다.

### RAGs (검색 증강 생성)이 작동하는 방식

![RAGs가 작동하는 방식을 보여주는 그림](../../images/how-rag-works.png?WT.mc_id=academic-105485-koreyst)

예를 들어, 노트에서 퀴즈를 생성하는 챗봇을 배포하려면 지식 기반과의 연결이 필요합니다. 이때 RAG가 그 해결책이 됩니다. RAG는 다음과 같이 작동합니다:

- **지식 기반:** 검색하기 전에, 이 문서들은 소화되고 전처리를 거쳐야 합니다. 일반적으로 큰 문서를 더 작은 청크로 나누고, 이를 텍스트 임베딩으로 변환한 후 데이터베이스에 저장합니다.

- **사용자 쿼리:** 사용자가 질문을 합니다.

- **검색:** 사용자가 질문을 하면, 임베딩 모델이 지식 기반에서 관련 정보를 검색하여 더 많은 컨텍스트를 제공하고 이를 프롬프트에 통합합니다.

- **증강 생성:** LLM은 검색된 데이터를 기반으로 응답을 개선합니다. 이는 미리 학습된 데이터뿐만 아니라 추가된 컨텍스트의 관련 정보에 기반하여 응답을 생성할 수 있게 합니다. 검색된 데이터는 LLM의 응답을 증강시키는 데 사용됩니다. LLM은 그런 다음 사용자의 질문에 대한 답을 반환합니다.

![RAGs 아키텍처를 보여주는 그림](../../images/encoder-decode.png?WT.mc_id=academic-105485-koreyst)

RAGs의 아키텍처는 인코더와 디코더의 두 부분으로 구성된 트랜스포머를 사용해 구현됩니다. 예를 들어, 사용자가 질문을 하면 입력 텍스트는 단어의 의미를 포착하는 벡터로 '인코딩'되고, 이 벡터는 문서 인덱스로 '디코딩'되어 사용자 쿼리에 기반하여 새로운 텍스트를 생성합니다. LLM은 출력 생성을 위해 인코더-디코더 모델을 모두 사용합니다.

다음은 논문 [Retrieval-Augmented Generation for Knowledge intensive NLP (지식 집약적 자연어 처리 작업을 위한 검색 증강 생성)](https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst)에서 제안된 RAG 구현의 두 가지 접근 방식입니다:

- **_RAG-Sequence_** 는 검색된 문서를 사용하여 사용자 쿼리에 대한 최적의 답변을 예측합니다.

- **RAG-Token** 은 문서를 사용하여 다음 토큰을 생성한 후 이를 검색하여 사용자의 쿼리에 답변합니다.

### 왜 RAG를 사용해야 할까요?

- **정보 풍부성:** 텍스트 응답이 최신 상태이므로 도메인 특정 작업에서 성능을 향상시킵니다. 내부 지식 베이스에 접근하여 이를 가능하게 합니다.

- **검증 가능한 데이터**를 지식 베이스에 활용하여 사용자 쿼리에 대한 문맥을 제공함으로써 허구를 줄입니다.

- LLM을 미세 조정하는 것과 비교했을 때 **비용 효율성**이 높아 경제적입니다.

## 지식 기반 만들기

우리의 애플리케이션은 개인 데이터, 즉 AI For Beginners 커리큘럼의 신경망 수업을 기반으로 합니다.

### 벡터 데이터베이스

벡터 데이터베이스는 전통적인 데이터베이스와 달리, 내장된 벡터를 저장, 관리 및 검색하기 위한 특화된 데이터베이스입니다. 이것은 문서의 수치적 표현을 저장합니다. 데이터를 수치적 임베딩으로 분해하면 AI 시스템이 데이터를 이해하고 처리하기 더 쉬워집니다.

벡터 데이터베이스에 임베딩을 저장하는 이유는 LLM이 입력으로 받아들일 수 있는 토큰 수에 한계가 있기 때문입니다. 전체 임베딩을 LLM에 전달할 수 없으므로 이를 청크로 나누어야 하며, 사용자가 질문을 하면 질문과 가장 유사한 임베딩이 프롬프트와 함께 반환됩니다. 청크를 나누면 LLM을 통과하는 토큰 수가 줄어들어 비용도 절감됩니다.

인기 있는 벡터 데이터베이스로는 Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant 및 DeepLake가 있습니다. 다음 명령어를 사용하여 Azure CLI로 Azure Cosmos DB 모델을 생성할 수 있습니다:

```bash
az login
az group create -n <resource-group-name> -l <location>
az cosmosdb create -n <cosmos-db-name> -r <resource-group-name>
az cosmosdb list-keys -n <cosmos-db-name> -g <resource-group-name>
```

### 텍스트에서 임베딩으로

데이터를 저장하기 전에 데이터베이스에 저장하기 위해 벡터 임베딩으로 변환해야 합니다. 대규모 문서나 긴 텍스트를 다룰 때는 예상되는 쿼리에 따라 청크로 나눌 수 있습니다. 청크로 나누는 작업은 문장 단위 또는 단락 단위로 할 수 있습니다. 청크는 주변 단어에서 의미를 유추하기 때문에 문서 제목을 추가하거나 청크 전후에 일부 텍스트를 포함하는 등의 다른 맥락을 추가할 수 있습니다. 데이터를 다음과 같이 청크로 나눌 수 있습니다:

```python
def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) < max_length and len(' '.join(current_chunk)) > min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # 마지막 청크가 최소 길이에 도달하지 못한 경우에도 추가
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
```

청크로 나눈 후에는 다양한 임베딩 모델을 사용하여 텍스트를 임베딩할 수 있습니다. 사용할 수 있는 모델에는 word2vec, OpenAI의 ada-002, Azure Computer Vision 등 다양한 모델이 있습니다. 사용할 모델을 선택할 때는 사용하는 언어, 인코딩된 콘텐츠 유형(텍스트/이미지/오디오), 인코딩할 수 있는 입력 크기 및 임베딩 출력 길이를 고려해야 합니다.

OpenAI의 `text-embedding-ada-002` 모델을 사용하여 텍스트를 임베딩한 예는 다음과 같습니다:
![고양이 단어의 임베딩](../../images/cat.png?WT.mc_id=academic-105485-koreyst)

## 검색 및 벡터 검색

사용자가 질문을 하면 검색기는 질의 인코더를 사용하여 질문을 벡터로 변환한 다음, 문서 검색 인덱스를 통해 입력과 관련된 벡터를 검색합니다. 검색이 완료되면 입력 벡터와 문서 벡터를 텍스트로 변환한 다음 이를 LLM에 전달합니다.

### 검색

검색은 시스템이 검색 기준을 충족하는 문서를 인덱스에서 빠르게 찾으려고 할 때 발생합니다. 검색기의 목표는 문서를 가져와서 LLM이 데이터에 대한 맥락을 제공하고 이를 기반으로 하는 것입니다.

우리의 데이터베이스에서 검색을 수행하는 몇 가지 방법은 다음과 같습니다:

- **키워드 검색** - 텍스트 검색에 사용됩니다.

- **시맨틱 검색** - 단어의 의미론적 의미를 사용합니다.

- **벡터 검색** - 문서를 임베딩 모델을 사용하여 텍스트에서 벡터 표현으로 변환합니다. 검색은 사용자의 질문과 가장 가까운 벡터 표현을 가진 문서를 쿼리하여 수행됩니다.

- **하이브리드** - 키워드 검색과 벡터 검색을 결합한 방식입니다.

검색에서의 도전 과제는 데이터베이스에 쿼리와 유사한 응답이 없을 때 발생합니다. 이 경우 시스템은 얻을 수 있는 최선의 정보를 반환하게 됩니다. 그러나 관련성의 최대 거리를 설정하거나 키워드와 벡터 검색을 결합한 하이브리드 검색을 사용하는 등의 전략을 사용할 수 있습니다. 이번 레슨에서는 벡터 검색과 키워드 검색을 결합한 하이브리드 검색을 사용할 것입니다. 우리는 데이터를 청크와 임베딩을 포함하는 열이 있는 데이터프레임에 저장할 것입니다.

### 벡터 유사성

리트리버는 지식 데이터베이스에서 가까운 임베딩을 검색합니다. 가장 가까운 이웃은 유사한 텍스트들입니다. 사용자가 쿼리를 요청하는 시나리오에서는 먼저 해당 쿼리가 임베딩되고, 유사한 임베딩과 매칭됩니다. 서로 다른 벡터가 얼마나 유사한지 찾는 데 주로 사용되는 측정 방법은 코사인 유사도로, 이는 두 벡터 사이의 각도에 기반합니다.

우리는 유사성을 측정하는 대체 방법으로 유클리드 거리와 내적을 사용할 수 있습니다. 유클리드 거리는 벡터 끝점 사이의 직선 거리이고, 내적은 두 벡터의 대응 요소의 곱의 합을 측정합니다.

### 검색 인덱스

검색을 수행할 때, 검색을 수행하기 전에 지식 베이스에 대한 검색 인덱스를 구축해야 합니다. 인덱스는 우리의 임베딩을 저장하고 큰 데이터베이스에서도 가장 유사한 청크를 신속하게 검색할 수 있습니다. 우리는 로컬에서 다음을 사용하여 인덱스를 만들 수 있습니다:

```python
from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()



# 검색 인덱스 생성
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)



# 인덱스를 쿼리하려면 kneighbors 메서드를 사용할 수 있습니다
distances, indices = nbrs.kneighbors(embeddings)
```

### 재랭킹

데이터베이스에 쿼리한 후에는 가장 관련성 있는 결과부터 정렬해야 할 수도 있습니다. 재랭킹 LLM은 기계 학습을 활용하여 검색 결과의 관련성을 개선하고 가장 관련성 있는 결과부터 정렬합니다. Azure AI Search를 사용하면, 의미론적 재랭커를 통해 자동으로 재랭킹이 수행됩니다. 가장 가까운 이웃을 사용한 재랭킹의 예:

````python

# 가장 유사한 문서 찾기
distances, indices = nbrs.kneighbors([query_vector])

index = []


# 가장 유사한 문서 출력
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f"데이터프레임에서 인덱스 {index}을(를) 찾을 수 없습니다")


## 모든 것을 종합하기

마지막 단계는 우리의 데이터에 기반을 둔 응답을 얻을 수 있도록 LLM을 추가하는 것입니다. 이를 다음과 같이 구현할 수 있습니다:

```python
user_input = "perceptron이란 무엇인가요?"

def chatbot(user_input):
    # 질문을 쿼리 벡터로 변환
    query_vector = create_embeddings(user_input)

    # 가장 유사한 문서 찾기
    distances, indices = nbrs.kneighbors([query_vector])

    # 쿼리에 문서 추가하여 문맥 제공
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # 히스토리와 사용자 입력 결합
    history.append(user_input)

    # 메시지 객체 생성
    messages = [
        {"role": "system", "content": "당신은 AI 질문에 도움을 주는 AI 어시스턴트입니다."},
        {"role": "user", "content": history[-1]}
    ]

    # 채팅 완료를 통해 응답 생성
    response = openai.chat.completions.create(
        model="gpt-4",
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
````

## 애플리케이션 평가하기

### 평가 지표

- 응답의 품질: 자연스럽고 유창하며 사람처럼 들리는지 확인

- 데이터의 근거성: 제공된 문서에서 나온 응답인지 평가

- 관련성: 응답이 질문과 일치하고 관련 있는지 평가

- 유창성: 응답이 문법적으로 의미가 있는지 평가

## RAG(검색 증강 생성) 및 벡터 데이터베이스의 사용 사례

다음과 같은 다양한 사용 사례에서 함수 호출을 통해 앱을 개선할 수 있습니다:

- 질문과 답변: 직원들이 질문할 수 있는 대화형 채팅에 회사 데이터를 기반으로 활용.
- 추천 시스템: 영화, 레스토랑 등 가장 유사한 값을 매칭하는 시스템 생성.
- 챗봇 서비스: 채팅 기록을 저장하고 사용자 데이터를 기반으로 대화를 개인화.
- 벡터 임베딩을 기반으로 한 이미지 검색, 이미지 인식 및 이상 탐지에 유용.

## 요약

우리는 애플리케이션에 데이터를 추가하는 것부터 사용자 쿼리와 출력까지 RAG의 기본적인 영역을 다루었습니다. RAG 생성의 단순화를 위해 Semanti Kernel, Langchain 또는 Autogen과 같은 프레임워크를 사용할 수 있습니다.

## 과제

검색 증강 생성(RAG)에 대한 학습을 계속하기 위해 다음을 빌드하세요:

- 선택한 프레임워크를 사용하여 애플리케이션의 프론트 엔드 빌드

- LangChain 또는 Semantic Kernel 프레임워크를 활용하고 애플리케이션을 재작성

레슨을 완료한 것을 축하드립니다 👏.

## 학습은 여기서 끝나지 않습니다, 여정을 계속하세요

이 수업을 완료한 후, [Generative AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI 지식을 계속해서 향상시켜보세요!

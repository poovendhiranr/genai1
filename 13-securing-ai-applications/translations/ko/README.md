# 생성형 AI 애플리케이션 보안

[![생성형 AI 애플리케이션 보안](../../images/13-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst)

## 소개

이 수업에서는 다음을 다룹니다:

- AI 시스템의 맥락에서 보안.
- AI 시스템에 대한 일반적인 위험과 위협.
- AI 시스템을 보호하기 위한 방법 및 고려사항.

## 학습 목표

이 수업을 마친 후, 다음을 이해하게 될 것입니다:

- AI 시스템에 대한 위협과 위험.
- AI 시스템을 보호하기 위한 일반적인 방법과 관행.
- 보안 테스트를 구현하여 예기치 않은 결과와 사용자 신뢰의 저하를 방지하는 방법.

## 생성형 AI의 맥락에서 보안이란 무엇인가?

인공지능(AI) 및 머신 러닝(ML) 기술이 점점 더 우리의 삶을 형성함에 따라, 고객 데이터를 보호하는 것뿐만 아니라 AI 시스템 자체를 보호하는 것도 중요합니다. AI/ML은 많은 산업에서 중요한 의사결정 과정에 점점 더 많이 사용되고 있으며, 잘못된 결정은 심각한 결과를 초래할 수 있습니다.

여기서 고려해야 할 주요 포인트는 다음과 같습니다:

- **AI/ML의 영향**: AI/ML은 일상 생활에 큰 영향을 미치므로 이를 보호하는 것이 필수적입니다.
- **보안 과제**: AI/ML의 이러한 영향에 맞춰 AI 기반 제품을 정교한 공격에서 보호하는 것이 중요합니다. 공격은 트롤러와 조직된 그룹 모두에 의해 발생할 수 있습니다.
- **전략적 문제**: 기술 산업은 장기적인 고객 안전 및 데이터 보안을 보장하기 위해 전략적 도전을 적극적으로 해결해야 합니다.

또한 머신 러닝 모델은 악의적인 입력과 benign한 이상 데이터를 구별하는 데 대부분 능숙하지 않습니다. 훈련 데이터의 주요 출처는 비검토, 비조정된 공공 데이터셋에서 비롯되며, 이는 3자인 누구나 기여할 수 있습니다. 공격자는 데이터를 침해할 필요도 없이 자유롭게 기여할 수 있습니다. 시간이 지남에 따라, 신뢰도가 낮은 악성 데이터는 데이터 구조나 형식이 올바른 한, 신뢰도가 높은 데이터로 변모할 수 있습니다.

이것이 바로 모델이 결정을 내리는데 사용하는 데이터 저장소의 무결성과 보호를 보장하는 것이 중요한 이유입니다.

## AI의 위협과 리스크 이해

AI 및 관련 시스템 측면에서, 데이터 포이즈닝(data poisoning)은 오늘날 가장 중요한 보안 위협으로 부각되고 있습니다. 데이터 포이즈닝은 누군가가 AI를 훈련시키는 데 사용되는 정보를 의도적으로 변경하여, AI가 실수를 저지르게 만드는 것입니다. 이는 표준화된 탐지 및 완화 방법이 결여되고, 신뢰할 수 없거나 검증되지 않은 공개 데이터셋에 의존하기 때문입니다. 데이터의 출처와 계보를 추적하는 것이 데이터 무결성을 유지하고 부정확한 훈련 과정을 방지하는 데 중요합니다. 그렇지 않으면 "쓰레기 들어가면, 쓰레기 나온다"는 옛말이 적용되어 모델 성능이 악화될 수 있습니다.

다음은 데이터 포이즈닝이 여러분의 모델에 어떻게 영향을 미칠 수 있는지에 대한 예시입니다:

1. **레이블 뒤집기**: 이진 분류 작업에서 상대방이 훈련 데이터의 일부 서브셋의 레이블을 고의로 뒤집습니다. 예를 들어, benign(양성) 샘플이 malicious(악성)으로 레이블되어 모델이 잘못된 연관을 학습하게 됩니다.\
   **예시**: 스팸 필터가 조작된 레이블로 인해 정상 이메일을 스팸으로 잘못 분류함.
2. **특성 포이즈닝**: 공격자가 훈련 데이터의 특성을 미세하게 수정하여 모델에 편향을 유도하거나 오도합니다.\
   **예시**: 추천 시스템을 조작하려고 제품 설명에 관련 없는 키워드를 추가함.
3. **데이터 주입**: 악성 데이터를 훈련 세트에 주입하여 모델의 동작을 영향을 미칩니다.\
   **예시**: 감정 분석 결과를 왜곡하기 위해 가짜 사용자 리뷰를 도입함.
4. **백도어 공격**: 상대방이 훈련 데이터에 숨겨진 패턴(백도어)을 삽입합니다. 모델은 이 패턴을 학습하여, 트리거되면 악의적으로 동작합니다.\
   **예시**: 특정 인물을 오인식하도록 백도어 이미지로 훈련된 얼굴 인식 시스템.

MITRE Corporation은 AI 시스템에 대한 실제 공격에서 적들이 사용하는 전술과 기술을 담은 [ATLAS(Adversarial Threat Landscape for Artificial-Intelligence Systems)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst)라는 지식 기반을 만들었습니다.

> AI 시스템의 통합이 증가함에 따라 기존의 사이버 공격 이외에도 AI를 활용한 시스템에 대한 취약점이 증가하고 있습니다. 우리는 전 세계 커뮤니티가 다양한 시스템에 AI를 점점 더 많이 통합함에 따라 이러한 독특하고 진화하는 취약점에 대한 인식을 높이기 위해 ATLAS를 개발했습니다. ATLAS는 MITRE ATT&CK® 프레임워크를 모델로 하여, ATT&CK의 전술, 기술, 절차(TTPs)와 보완적입니다.

전통적인 사이버 보안에서 고급 위협 에뮬레이션 시나리오를 계획하는 데 광범위하게 사용되는 MITRE ATT&CK® 프레임워크와 마찬가지로, ATLAS는 emerging attacks에 대비하기 위해 더 잘 이해하고 준비하는 데 도움이 되는 쉽게 검색 가능한 TTP 집합을 제공합니다.

또한, Open Web Application Security Project (OWASP)는 LLMs를 활용하는 애플리케이션에서 발견된 가장 중요한 취약점의 "[Top 10 리스트](https://llmtop10.com/?WT.mc_id=academic-105485-koreyst)"를 작성했습니다. 이 목록은 앞서 언급한 데이터 포이즈닝과 같은 위협과 다음과 같은 다른 위협의 리스크를 강조합니다:

- **프롬프트 인젝션**: 공격자가 주의 깊게 제작한 입력을 통해 대형 언어 모델(LLM)을 조작하여 의도된 동작을 벗어나게 하는 기술.
- **공급망 취약점**: Python 모듈이나 외부 데이터셋과 같은 LLMs에서 사용되는 애플리케이션 구성 요소와 소프트웨어가 자체적으로 손상되어 예상치 못한 결과, 편향을 도입하거나 기본 인프라의 취약점을 초래할 수 있음.
- **과도한 의존**: LLMs는 오류를 범할 수 있으며 부정확하거나 안전하지 않은 결과를 제공할 가능성이 큽니다. 여러 문서화된 상황에서 사람들이 결과를 곧이곧대로 받아들여 의도치 않은 실제 세계의 부정적 결과로 이어진 경우가 있습니다.

Microsoft Cloud Advocate Rod Trent는 이러한 신흥 AI 위협에 대해 깊이 탐구하고 이러한 시나리오를 최적의 방법으로 다루는 광범위한 지침을 제공하는 무료 전자책 [Must Learn AI Security](https://github.com/rod-trent/OpenAISecurity/tree/main/Must_Learn/Book_Version?WT.mc_id=academic-105485-koreyst)를 저술했습니다.

## AI 시스템 및 LLM의 보안 테스트

인공지능(AI)은 다양한 분야와 산업을 변혁시키며 사회에 새로운 가능성과 혜택을 제공하고 있습니다. 그러나 AI는 데이터 프라이버시, 편향성, 설명 가능성 부족, 잠재적 오용 등의 상당한 문제와 위험도 제기합니다. 따라서 AI 시스템이 윤리적 및 법적 기준을 준수하고 사용자 및 이해관계자가 신뢰할 수 있도록 보안되고 책임 있게 만들어지는 것이 중요합니다.

보안 테스트는 AI 시스템 또는 LLM(대규모 언어 모델)의 보안을 평가하여 취약점을 식별하고 이를 악용하는 과정입니다. 이는 테스트의 목적과 범위에 따라 개발자, 사용자 또는 서드파티 감사자에 의해 수행될 수 있습니다. AI 시스템 및 LLM을 위한 가장 일반적인 보안 테스트 방법은 다음과 같습니다:

- **데이터 소독**: 이는 AI 시스템 또는 LLM의 학습 데이터 또는 입력에서 민감하거나 개인적인 정보를 제거하거나 익명화하는 과정입니다. 데이터 소독은 비밀 또는 개인 데이터의 노출을 줄여 데이터 유출과 악의적 조작을 방지하는 데 도움이 됩니다.
- **적대적 테스트**: 이는 AI 시스템 또는 LLM의 입력 또는 출력에 적대적 예제를 생성하고 적용하여 그 견고성과 적대적 공격에 대한 저항력을 평가하는 과정입니다. 적대적 테스트는 공격자가 악용할 수 있는 AI 시스템 또는 LLM의 취약점과 약점을 식별하고 완화하는 데 도움이 됩니다.
- **모델 검증**: 이는 AI 시스템 또는 LLM의 모델 매개변수 또는 아키텍처의 정확성과 완전성을 검증하는 과정입니다. 모델 검증은 모델이 보호되고 인증되어 모델 도난을 방지하는지 확인하는 데 도움이 됩니다.
- **출력 검증**: 이는 AI 시스템 또는 LLM의 출력 품질과 신뢰성을 검증하는 과정입니다. 출력 검증은 출력이 일관되고 정확한지 확인하여 악의적 조작을 감지하고 수정하는 데 도움이 됩니다.

OpenAI는 첨단 AI 시스템의 선두 주자로, AI 안전에 기여하고자 하는 목표로 붉은 팀 네트워크 이니셔티브의 일환으로 일련의 *안전 평가*를 설정했습니다. 이 평가는 AI 시스템의 출력을 테스트하기 위한 것입니다.

> 평가의 범위는 간단한 Q&A 테스트에서 더 복잡한 시뮬레이션까지 다양할 수 있습니다. 다음은 다양한 각도에서 AI 행동을 평가하기 위해 OpenAI가 개발한 샘플 평가의 구체적인 예입니다:

#### 설득

- [MakeMeSay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_say/readme.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 다른 AI 시스템을 비밀 단어를 말하게 속이는 능력.
- [MakeMePay](https://github.com/openai/evals/tree/main/evals/elsuite/make_me_pay/readme.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 다른 AI 시스템을 돈을 기부하도록 설득하는 능력.
- [Ballot Proposal](https://github.com/openai/evals/tree/main/evals/elsuite/ballots/readme.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 다른 AI 시스템의 정치적 제안에 대한 지지에 영향을 미치는 능력.

#### 스테가노그래피(은밀한 메시징)

- [스테가노그래피](https://github.com/openai/evals/tree/main/evals/elsuite/steganography/readme.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 다른 AI 시스템에 의해 잡히지 않고 비밀 메시지를 얼마나 잘 전달할 수 있는가?
- [텍스트 압축](https://github.com/openai/evals/tree/main/evals/elsuite/text_compression/readme.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 메시지를 압축 및 해제압축하여 비밀 메시지를 숨기는 능력을 얼마나 잘 갖추고 있는가?
- [셸링 포인트](https://github.com/openai/evals/blob/main/evals/elsuite/schelling_point/README.md?WT.mc_id=academic-105485-koreyst): AI 시스템이 직접적인 의사소통 없이 다른 AI 시스템과 얼마나 잘 협력할 수 있는가?

### AI 보안

AI 시스템을 악의적인 공격, 남용 또는 의도하지 않은 결과로부터 보호하는 것이 중요합니다. 여기에는 AI 시스템의 안전성, 신뢰성 및 신뢰성을 보장하기 위한 다음과 같은 조치가 포함됩니다:

- AI 모델을 훈련하고 실행하는 데 사용되는 데이터 및 알고리즘의 보안
- AI 시스템의 무단 액세스, 조작 또는 사보타주 방지
- AI 시스템의 편향성, 차별 또는 윤리적 문제 감지 및 완화
- AI 결정과 행동의 책임성, 투명성 및 설명 가능성 보장
- AI 시스템의 목표와 가치를 인간과 사회의 목표와 가치에 맞추기

AI 보안은 AI 시스템과 데이터의 무결성, 가용성 및 기밀성을 보장하는 데 중요합니다. AI 보안의 도전과 기회는 다음과 같습니다:

- 기회: 사이버 보안 전략에 AI를 통합하면 위협을 식별하고 대응 시간을 개선하는 데 중요한 역할을 할 수 있습니다. AI는 피싱, 악성 소프트웨어 또는 랜섬웨어와 같은 사이버 공격의 감지 및 완화를 자동화하고 증강하는 데 도움을 줄 수 있습니다.
- 도전: AI는 적대자에 의해 정교한 공격을 실행하는 데도 사용될 수 있습니다. 예를 들어, 가짜 또는 오해를 불러일으키는 콘텐츠 생성, 사용자 사칭 또는 AI 시스템의 취약성 악용 등이 있습니다. 따라서 AI 개발자는 남용에 강한 견고하고 탄력적인 시스템을 설계할 책임이 있습니다.

### 데이터 보호

LLM은 사용하는 데이터의 프라이버시와 보안에 위험을 초래할 수 있습니다. 예를 들어, LLM은 개인 이름, 주소, 비밀번호, 신용카드 번호와 같은 훈련 데이터에서 민감한 정보를 기억하고 유출할 가능성이 있습니다. 또한, 악의적인 행위자가 취약점이나 편향성을 악용하기 위해 LLM을 조작하거나 공격할 수 있습니다. 따라서 이러한 위험을 인지하고 LLM과 함께 사용하는 데이터를 보호하기 위한 적절한 조치를 취하는 것이 중요합니다. 데이터를 보호하기 위해 취할 수 있는 몇 가지 단계는 다음과 같습니다:

- **LLM과 공유하는 데이터의 양과 유형 제한**: 필요한 목적에 적합하고 관련된 데이터만 공유하며, 민감하거나 기밀이거나 개인적인 데이터는 공유하지 않도록 해야 합니다. 사용자는 또한 식별 정보를 제거하거나 마스킹하거나, 안전한 통신 채널을 사용하여 LLM과 공유하는 데이터를 익명화하거나 암호화하여야 합니다.
- **LLM이 생성한 데이터 검증**: LLM이 생성한 출력물이 원치 않거나 부적절한 정보를 포함하지 않는지 항상 정확성과 품질을 확인해야 합니다.
- **데이터 유출이나 사건의 보고 및 경고**: LLM이 생성하는 텍스트가 부적절, 부정확, 공격적, 해로운 경우와 같은 의심스러운 활동이나 비정상적인 행동을 주의 깊게 관찰해야 합니다. 이는 데이터 유출이나 보안 사고의 징후일 수 있습니다.

데이터 보안, 거버넌스, 준수는 다중 클라우드 환경에서 데이터와 AI의 힘을 활용하고자 하는 모든 조직에 필수적입니다. 모든 데이터를 보호하고 관리하는 것은 복잡하고 다면적인 작업입니다. 구조화된 데이터, 비구조화된 데이터, AI가 생성한 데이터를 포함하여 여러 클라우드의 여러 위치에 있는 다양한 유형의 데이터를 보호하고 관리해야 하며, 현재 및 미래의 데이터 보안, 거버넌스, AI 규제를 고려해야 합니다. 데이터를 보호하기 위해 다음과 같은 모범 사례 및 예방 조치를 채택해야 합니다:

- 데이터 보호 및 프라이버시 기능을 제공하는 클라우드 서비스 또는 플랫폼 사용.
- 데이터 오류, 불일치 또는 이상을 확인하기 위한 데이터 품질 및 검증 도구 사용.
- 데이터가 책임감 있고 투명하게 사용되도록 보장하기 위한 데이터 거버넌스 및 윤리 프레임워크 사용.

### 실제 세계의 위협을 모방하기 - AI 레드 팀

실제 세계의 위협을 모방하는 것은 시스템의 취약점을 식별하고 수비자의 대응을 테스트하기 위해 유사한 도구, 전술, 절차를 사용하는 것을 통해 탄력적인 AI 시스템을 구축하는 표준 관행으로 간주됩니다.

> AI 레드 팀 실행의 의미는 진화하여 더 확장된 의미를 가지게 되었습니다: 이는 보안 취약점을 탐색하는 것뿐만 아니라 잠재적으로 해로운 콘텐츠 생성을 포함한 다른 시스템 실패를 탐색하는 것도 포함합니다. AI 시스템은 새로운 위험을 가지고 있으며, 레드 팀은 프롬프트 주입 및 기반 없는 콘텐츠 생성과 같은 새로운 위험을 이해하는 데 중요합니다. - [Microsoft AI 레드 팀이 더 안전한 AI의 미래를 구축](https://www.microsoft.com/security/blog/2023/08/07/microsoft-ai-red-team-building-future-of-safer-ai/?WT.mc_id=academic-105485-koreyst)

[![레드 팀에 대한 지침 및 리소스](../../images/13-AI-red-team.png?WT.mc_id=academic-105485-koreyst)]()

아래는 Microsoft의 AI 레드 팀 프로그램을 형성한 주요 통찰입니다.

1. **AI 레드 팀의 광범위한 범위:**
   AI 레드 팀은 이제 보안과 책임 있는 AI(RAI) 결과 모두를 포괄합니다. 전통적으로 레드 팀은 모델을 벡터로 취급하고 보안 측면에 초점을 맞췄습니다(예: 기본 모델 탈취). 그러나 AI 시스템은 새로운 보안 취약점을 도입합니다(예: 프롬프트 주입, 데이터 중독), 이는 특별한 주의가 필요합니다. 보안을 넘어, AI 레드 팀은 공정성 이슈(예: 고정관념)와 유해한 콘텐츠(예: 폭력 미화)도 탐험합니다. 이러한 문제를 조기에 식별하면 방어 투자 우선순위를 정할 수 있습니다.
2. **악의적이고 무해한 실패:**
   AI 레드 팀은 악의적이거나 무해한 관점에서의 실패를 고려합니다. 예를 들어, 새로운 Bing을 레드 팀 할 때, 악의적인 공격자가 시스템을 전복시키는 방법뿐만 아니라 일반 사용자가 문제나 유해한 콘텐츠를 만나는 상황도 탐구합니다. 전통적인 보안 레드 팀이 주로 악의적인 행위자에 집중하는 것과 달리, AI 레드 팀은 더 넓은 범위의 인물과 잠재적인 실패를 고려합니다.
3. **AI 시스템의 동적 특성:**
   AI 애플리케이션은 끊임없이 진화합니다. 대형 언어 모델 애플리케이션에서는 개발자들이 변화하는 요구 사항에 맞추어 적응합니다. 지속적인 레드 팀 활동은 진화하는 위험에 대한 지속적인 경계와 적응을 보장합니다.

AI 레드 팀은 보완적 동작으로 간주하여 [역할 기반 접근 제어 (RBAC)](https://learn.microsoft.com/azure/ai-services/openai/how-to/role-based-access-control?WT.mc_id=academic-105485-koreyst) 및 종합적인 데이터 관리 솔루션과 같은 추가 통제 수단을 보완해줘야 합니다. 이는 프라이버시와 보안을 중시하면서 편견, 유해한 콘텐츠, 잘못된 정보를 최소화하여 사용자 신뢰를 훼손하지 않는 안전하고 책임 있는 AI 솔루션을 구현하는 것을 목표로 하는 보안 전략을 보완하기 위함입니다.

AI 시스템에서 레드 팀이 위험을 식별하고 완화하는 데 도움이 되는 추가 읽을거리 목록은 다음과 같습니다:

- [대형 언어 모델(LLM) 및 그 애플리케이션에 대한 레드 팀 계획](https://learn.microsoft.com/azure/ai-services/openai/concepts/red-teaming?WT.mc_id=academic-105485-koreyst)
- [OpenAI 레드 팀 네트워크란?](https://openai.com/blog/red-teaming-network?WT.mc_id=academic-105485-koreyst)
- [AI 레드 팀 - 더 안전하고 더 책임 있는 AI 솔루션을 구축하는 핵심 관행](https://rodtrent.substack.com/p/ai-red-teaming?WT.mc_id=academic-105485-koreyst)
- MITRE [ATLAS (인공지능 시스템에 대한 적대적 위협 지형도)](https://atlas.mitre.org/?WT.mc_id=academic-105485-koreyst), AI 시스템에 대한 실제 공격에서 사용되는 전술과 기술이 기록된 지식 기반.

## 지식 확인

데이터 무결성을 유지하고 오용을 방지하기 위한 좋은 접근 방법은 무엇일까요?

1. 데이터 접근 및 관리에 강력한 역할 기반 제어를 적용하기
1. 데이터의 오인을 방지하거나 오용을 막기 위해 데이터 라벨링을 구현하고 감사하기
1. AI 인프라가 콘텐츠 필터링을 지원하도록 보장하기

A:1, 세 가지 모두 훌륭한 권장 사항이지만, 사용자에게 적절한 데이터 접근 권한을 부여하는 것이 LLM이 사용하는 데이터의 조작 및 오인을 방지하는 데 큰 도움이 됩니다.

## 🚀 도전

AI 시대에 민감한 정보를 [관리하고 보호하는 방법](https://learn.microsoft.com/training/paths/purview-protect-govern-ai/?WT.mc_id=academic-105485-koreyst)에 대해 더 알아보세요.

## 수고하셨습니다. 학습을 계속하세요!

이 레슨을 완료한 후 [Generative AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 Generative AI 지식을 계속 향상시키세요!

Lesson 14로 이동하여 [생성형 AI 애플리케이션 수명 주기](../../../14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)를 살펴보세요!

# LLM 세부 튜닝

대규모 언어 모델을 사용하여 생성 AI 애플리케이션을 구축하는 데는 새로운 도전 과제가 따릅니다. 주요 문제는 주어진 사용자 요청에 대해 모델이 생성한 콘텐츠의 응답 품질(정확성 및 관련성)을 보장하는 것입니다. 이전 강의에서는 프롬프트 엔지니어링 및 검색 증강 생성과 같은 기술을 사용하여 기존 모델의 프롬프트 입력을 수정하여 문제를 해결하려고 했습니다.

오늘 강의에서는 **세부 튜닝**이라는 세 번째 기술에 대해 논의합니다. 이는 추가 데이터를 사용하여 모델 자체를 재훈련하여 문제를 해결하려고 합니다. 이제 세부 사항을 살펴보겠습니다.

## 학습 목표

이 레슨은 사전 학습된 언어 모델을 미세 조정하는 개념을 소개하고, 이 접근 방식의 이점과 어려움을 탐구하며, 생성된 AI 모델의 성능을 향상시키기 위해 언제 그리고 어떻게 미세 조정을 사용할지에 대한 지침을 제공합니다.

이 레슨이 끝날 때까지 다음 질문에 답할 수 있어야 합니다:

- 언어 모델의 미세 조정이란 무엇인가요?
- 언제, 그리고 왜 미세 조정이 유용한가요?
- 사전 학습된 모델을 어떻게 미세 조정할 수 있나요?
- 미세 조정의 한계는 무엇인가요?

준비되셨나요? 시작해봅시다.

## 삽화 가이드

정식으로 들어가기 전에 이번 강의에서 다룰 내용을 한눈에 보고 싶으신가요? 핵심 개념과 미세 조정의 동기에 대해 배우는 것부터, 그 과정을 이해하고 최적의 실행 방법을 학습하는 여정까지 설명하는 이 삽화 가이드를 확인해보세요. 탐구하기에 흥미로운 주제이니, 자기 주도적 학습 여정을 지원하기 위해 추가 링크가 있는 [자료](../../RESOURCES.md?WT.mc_id=academic-105485-koreyst) 페이지도 잊지 말고 확인하세요!

![언어 모델 미세 조정에 대한 삽화 가이드](../../img/18-fine-tuning-sketchnote.png?WT.mc_id=academic-105485-koreyst)

## 언어 모델의 파인튜닝이란 무엇인가?

정의에 따르면, 대형 언어 모델은 인터넷을 포함한 다양한 출처로부터 수집된 대량의 텍스트로 **사전 학습**(pre-trained)됩니다. 이전 강의에서 배운 바와 같이, 모델이 사용자 질문(프롬프트)에 대한 응답의 질을 높이기 위해 **프롬프트 엔지니어링**(prompt engineering)과 **검색 증강 생성**(retrieval-augmented generation) 같은 기술이 필요합니다.

인기 있는 프롬프트 엔지니어링 기법 중 하나는 모델에게 응답에 기대되는 것을 더 많이 안내하는 것입니다. 이를 위해 **명시적 안내**(explicit guidance)로 **지시사항**을 제공하거나, **암시적 안내**(implicit guidance)로 **몇 가지 예시**를 제공할 수 있습니다. 이는 **몇-샷 학습**(few-shot learning)이라고 부르지만 두 가지 한계가 있습니다:

- 모델 토큰 제한은 제공할 수 있는 예제 수를 제한하고, 효과를 제한할 수 있습니다.
- 모델 토큰 비용은 모든 프롬프트에 예제를 추가하는 데 비용이 많이 들 수 있고, 유연성을 제한할 수 있습니다.

파인튜닝은 기계 학습 시스템에서 일반적으로 사용되는 방법으로, 사전 학습된 모델을 새 데이터로 재학습시켜 특정 작업의 성능을 향상시키는 것입니다. 언어 모델의 경우, 사전 학습된 모델을 **특정 작업 또는 응용 도메인에 대한 잘 선별된 예제 집합**으로 파인튜닝하여, 해당 특정 작업이나 도메인에 더 정확하고 관련성 있는 **맞춤형 모델**을 생성할 수 있습니다. 파인튜닝의 부가적인 이점은 몇-샷 학습에 필요한 예제 수를 줄여서 토큰 사용량과 관련 비용을 줄일 수 있다는 점입니다.

## 언제 그리고 왜 모델을 미세 조정해야 하는가?

_이_ 맥락에서, 미세 조정(fine-tuning)에 대해 이야기할 때, 우리는 원래의 학습 데이터셋에 포함되지 않았던 **새로운 데이터를 추가**하여 다시 학습시키는 **지도형 미세 조정**을 의미합니다. 이는 모델을 원래의 데이터로, 그러나 다른 하이퍼파라미터로 다시 학습시키는 비지도형 미세 조정 접근법과는 다릅니다.

기억해야 할 중요한 점은 미세 조정이 원하는 결과를 얻기 위해 일정 수준의 전문 지식이 필요한 고급 기술이라는 것입니다. 올바르게 수행되지 않으면 기대한 향상을 제공하지 못할 수도 있고, 심지어 목표로 하는 도메인에서 모델의 성능을 저하시킬 수도 있습니다.

따라서, 언어 모델을 "어떻게" 미세 조정해야 하는지 배우기 전에, "왜" 이 길을 선택해야 하는지와 "언제" 미세 조정을 시작해야 하는지를 알아야 합니다. 다음 질문들을 던져 보세요:

- **사용 사례**: 미세 조정의 *사용 사례*는 무엇인가요? 현재 사전 학습된 모델의 어떤 측면을 개선하고 싶으신가요?
- **대안**: 원하는 결과를 얻기 위해 *다른 기술들*을 시도해 보셨나요? 이를 사용하여 비교를 위한 기준을 만드세요.
  - 프롬프트 엔지니어링: 관련 프롬프트 응답의 예제를 사용한 몇 샷 프롬프트 기법을 시도하세요. 응답의 품질을 평가합니다.
  - 검색 증강 생성(RAG): 데이터를 검색하여 검색된 쿼리 결과로 프롬프트를 증강하는 것을 시도해 보세요. 응답의 품질을 평가합니다.
- **비용**: 미세 조정의 비용을 파악하셨나요?
  - 조정 가능성 - 사전 학습된 모델이 미세 조정 가능한가요?
  - 노력 - 학습 데이터를 준비하고 모델을 평가하고 개선하는 데 필요한 노력.
  - 컴퓨팅 - 미세 조정 작업을 실행하고 미세 조정된 모델을 배포하는 데 필요한 컴퓨팅 자원.
  - 데이터 - 충분한 품질의 예제를 미세 조정 영향을 위해 접근할 수 있는가?
- **혜택**: 미세 조정의 혜택을 확인하셨나요?
  - 품질 - 미세 조정된 모델이 기준 모델을 능가하였나요?
  - 비용 - 프롬프트를 단순화하여 토큰 사용을 줄이는가?
  - 확장성 - 베이스 모델을 새로운 도메인에 다시 사용할 수 있는가?

이 질문들에 답함으로써 미세 조정이 귀하의 사용 사례에 적절한지 결정할 수 있습니다. 이상적으로, 접근 방식은 비용보다 혜택이 클 때만 유효합니다. 진행하기로 결정했다면 이제는 사전 학습된 모델을 _어떻게_ 미세 조정할 수 있을지에 대해 생각해 볼 때입니다.

결정 과정을 더 알고 싶으신가요? [미세 조정을 할지 말지 고민 중이라면?](https://www.youtube.com/watch?v=0Jo-z-MFxJs)을 시청하세요.

## 사전 학습된 모델을 미세 조정하는 방법

사전 학습된 모델을 미세 조정하려면 다음이 필요합니다:

- 미세 조정할 사전 학습된 모델
- 미세 조정에 사용할 데이터셋
- 미세 조정 작업을 수행할 수 있는 학습 환경
- 미세 조정된 모델을 배포할 호스팅 환경

## 맞춤형 모델 학습 실제 사례

다음 자원들은 선택한 모델과 큐레이션된 데이터 셋을 사용하여 실제 예제를 단계별로 안내하는 튜토리얼을 제공합니다. 이 튜토리얼을 진행하려면 특정 제공업체에서 계정이 필요하며 관련 모델과 데이터 셋에 접근할 수 있어야 합니다.

| 제공업체     | 튜토리얼                                                                                                                                                                       | 설명                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [챗 모델 맞춤형 학습 방법](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                    | `gpt-35-turbo` 모델을 특정 도메인(예: "레시피 도우미")에 맞춰 학습하기 위해 학습 데이터를 준비하고, 맞춤형 학습 작업을 실행하며, 맞춤형 학습된 모델을 추론에 사용하는 법을 배웁니다.                                                                                                                                                                                                                                                   |
| Azure OpenAI | [GPT 3.5 Turbo 맞춤형 학습 튜토리얼](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | **Azure**에서 `gpt-35-turbo-0613` 모델을 맞춤형 학습시키는 절차를 배우며, 학습 데이터 생성 및 업로드, 맞춤형 학습 작업 실행, 새로운 모델 배포 및 사용에 대해 다룹니다.                                                                                                                                                                                                                                                                 |
| Hugging Face | [Hugging Face를 활용한 대규모 언어 모델 맞춤형 학습](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                               | 이 블로그 게시물은 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 라이브러리 및 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst)를 사용하여 공공 [데이터 셋](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst)으로 _공용 LLM_(예: `CodeLlama 7B`)을 맞춤형 학습시키는 과정을 안내합니다. |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| 🤗 AutoTrain | [AutoTrain을 활용한 대규모 언어 모델 맞춤형 학습](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                         | AutoTrain(또는 AutoTrain Advanced)는 Hugging Face에서 개발된 파이썬 라이브러리로, 다양한 작업을 위한 맞춤형 학습을 지원합니다. AutoTrain은 코드 작성 없이도 맞춤형 학습을 수행할 수 있으며, 자체 클라우드, Hugging Face Spaces 또는 로컬 환경에서 사용할 수 있습니다. 웹 기반 GUI, CLI 및 yaml 구성 파일을 통한 학습을 지원합니다.                                                                                                     |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                        |

## 과제

위의 튜토리얼 중 하나를 선택하여 진행하세요. _참고용으로 이 저장소의 Jupyter 노트북에서 이러한 튜토리얼의 버전을 복제할 수 있습니다. 최신 버전을 사용하려면 원본 소스를 직접 사용하세요._

## 수고하셨습니다! 학습을 계속하세요.

이 레슨을 완료한 후 [Generative AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 Generative AI 지식을 계속 향상시킬 수 있습니다!

축하합니다!! 이 과정의 v2 시리즈의 마지막 레슨을 완료했습니다! 학습과 개발을 멈추지 마세요. \*\*[자료들](RESOURCES.md?WT.mc_id=academic-105485-koreyst) 페이지를 확인하여 이 주제에 대한 추가 제안을 확인하세요.

v1 시리즈의 레슨들도 더 많은 과제와 개념으로 업데이트되었습니다. 지식을 갱신하는 데 잠시 시간을 내세요 - 그리고 이 레슨들을 개선하는 데 도움을 주기 위해 [질문과 피드백을 공유](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)해 주세요.
